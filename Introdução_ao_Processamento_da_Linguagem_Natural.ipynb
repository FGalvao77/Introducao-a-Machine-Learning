{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Introdução ao Processamento da Linguagem Natural.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMThtp+HtZgeR16FKw/0eGX"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0-2zVEUxWmtP"
      },
      "source": [
        "# Introdução ao Processamento da Linguagem Natural\n",
        "\n",
        "A capacidade de comunicar-se é uma das principais características que nos permitem viver em sociedade. E a tecnologia é um aspecto que vem gradativamente evoluindo e facilitando a forma com que nos comunicamos.\n",
        "\n",
        "Nesse sentindo, está cada vez mais fácil usar aplicativos para troca de mensagens e já existem, inclusive, sistemas que usam machine learning para simular uma conversa como se fossem uma pessoa respondendo. Quanta inovação, não é mesmo?!\n",
        "\n",
        "Esses sistemas são chamados de chatbots. Eles já estão presentes em diversos sites de comércio eletrônico e em serviços de atendimento de várias empresas. Neles o usuário geralmente é apresentado a um chat com algumas opções de serviço e o assistente virtual vai direcionando o utilizador para determinado setor de atendimento da empresa, com base em suas respostas em mensagens de texto.\n",
        "\n",
        "E como esses sistemas fazem isso? Como eles conseguem entender as mensagens do usuário?\n",
        "\n",
        "Bem, por trás dessa tecnologia existe uma área da computação que é chamada de `Processamento da Linguagem Natural`, **tem o objetivo de fazer com que um sistema consiga entender a linguagem dos humanos, ou seja, entender o que nós escrevemos ou falamos.**\n",
        "\n",
        "O Processamento da Linguagem Natural, também representado pela sigla `PLN, ou NLP do inglês Natural Language Processing`, utiliza conceitos baseados em linguística e regras gramaticais para a construção de algoritmos, que possam extrair alguma informação ou\n",
        "entendimento.\n",
        "\n",
        "Dentre as aplicações do PNL pode-se destacar a sumarização ou resumo de textos, que permite, por exemplo, captar apenas as ideias principais que contém o sentido de um texto ou de um livro. Outra aplicação são os aplicativos de tradução que utilizam o reconhecimento de voz do usuário para traduzir uma frase. **O PNL pode ser aplicado ainda em recuperação de informação, chatbots, entre outras utilidades.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0KRIoEbgXcpg"
      },
      "source": [
        "Agora você será apresentado, de fato, a alguns conceitos importantes no aprendizado de PLN. São eles: `Corpus, Tokenização, e Stop words.`\n",
        "\n",
        "`O Corpus é representado por um conjunto de textos escritos em um determinado idioma, que foram manualmente anotados e servirá de validação para as análises que serão realizadas.`\n",
        "\n",
        "Já a `Tokenização, refere-se a divisão de um texto em partes menores que representam as palavras, ou também chamadas de tokens, e podem ser separadas por espaços, vírgulas ou pontuações.`\n",
        "\n",
        "Por último, mas não menos importante, `os Stop words. Eles são palavras que geralmente são removidas no início do processamento dos textos para acelerar esse processo, porém a retirada dessas palavras não afeta a compreensão do mesmo.`\n",
        "\n",
        "Após a definição destes conceitos, para que você os entenda melhor e consiga utilizá-los em seu cotidiano de programador é importante acompanhar a aplicação do que foi estudado.\n",
        "\n",
        "Nesse caso, será usando uma biblioteca do python chamada de **NLTK** que significa `Natural Language Toolkit`. Então, com o ambiente do jupyter aberto, faça inicialmente o download da biblioteca com o comando `“! pip install nltk”`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z9684HlxbEOb"
      },
      "source": [
        "**PARTE 1**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YGw8GKQ7WAcF",
        "outputId": "2211b8a4-b323-4d6f-f9c3-4051f6c51c9b"
      },
      "source": [
        "# realizando o donwload e instalação da biblioteca\n",
        "! pip install nltk"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (3.2.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk) (1.15.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "66nySj4EWcKe"
      },
      "source": [
        "# importando a biblioteca\n",
        "import nltk"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SZOnMwnXWcHA",
        "outputId": "86c1cb1a-7a0e-4b1e-fe0e-92927941b089"
      },
      "source": [
        "# importando o \"stopwords\"\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BzkMCJVxWcAH",
        "outputId": "5c16c741-b7f8-48e2-cdea-70cad09d465e"
      },
      "source": [
        "# instanciando o algoritmo na variável \"stopwords\"\n",
        "# e configurando para o obte-las em português\n",
        "stopwords = nltk.corpus.stopwords.words('portuguese')"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['de', 'a', 'o', 'que', 'e', 'é', 'do', 'da', 'em', 'um', 'para', 'com', 'não', 'uma', 'os', 'no', 'se', 'na', 'por', 'mais', 'as', 'dos', 'como', 'mas', 'ao', 'ele', 'das', 'à', 'seu', 'sua', 'ou', 'quando', 'muito', 'nos', 'já', 'eu', 'também', 'só', 'pelo', 'pela', 'até', 'isso', 'ela', 'entre', 'depois', 'sem', 'mesmo', 'aos', 'seus', 'quem', 'nas', 'me', 'esse', 'eles', 'você', 'essa', 'num', 'nem', 'suas', 'meu', 'às', 'minha', 'numa', 'pelos', 'elas', 'qual', 'nós', 'lhe', 'deles', 'essas', 'esses', 'pelas', 'este', 'dele', 'tu', 'te', 'vocês', 'vos', 'lhes', 'meus', 'minhas', 'teu', 'tua', 'teus', 'tuas', 'nosso', 'nossa', 'nossos', 'nossas', 'dela', 'delas', 'esta', 'estes', 'estas', 'aquele', 'aquela', 'aqueles', 'aquelas', 'isto', 'aquilo', 'estou', 'está', 'estamos', 'estão', 'estive', 'esteve', 'estivemos', 'estiveram', 'estava', 'estávamos', 'estavam', 'estivera', 'estivéramos', 'esteja', 'estejamos', 'estejam', 'estivesse', 'estivéssemos', 'estivessem', 'estiver', 'estivermos', 'estiverem', 'hei', 'há', 'havemos', 'hão', 'houve', 'houvemos', 'houveram', 'houvera', 'houvéramos', 'haja', 'hajamos', 'hajam', 'houvesse', 'houvéssemos', 'houvessem', 'houver', 'houvermos', 'houverem', 'houverei', 'houverá', 'houveremos', 'houverão', 'houveria', 'houveríamos', 'houveriam', 'sou', 'somos', 'são', 'era', 'éramos', 'eram', 'fui', 'foi', 'fomos', 'foram', 'fora', 'fôramos', 'seja', 'sejamos', 'sejam', 'fosse', 'fôssemos', 'fossem', 'for', 'formos', 'forem', 'serei', 'será', 'seremos', 'serão', 'seria', 'seríamos', 'seriam', 'tenho', 'tem', 'temos', 'tém', 'tinha', 'tínhamos', 'tinham', 'tive', 'teve', 'tivemos', 'tiveram', 'tivera', 'tivéramos', 'tenha', 'tenhamos', 'tenham', 'tivesse', 'tivéssemos', 'tivessem', 'tiver', 'tivermos', 'tiverem', 'terei', 'terá', 'teremos', 'terão', 'teria', 'teríamos', 'teriam']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "awXwqugWc31u",
        "outputId": "0ca11327-4588-4e45-b534-e42c7ba12d74"
      },
      "source": [
        "# imprimindo as stopwords em português\n",
        "print(stopwords)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['de', 'a', 'o', 'que', 'e', 'é', 'do', 'da', 'em', 'um', 'para', 'com', 'não', 'uma', 'os', 'no', 'se', 'na', 'por', 'mais', 'as', 'dos', 'como', 'mas', 'ao', 'ele', 'das', 'à', 'seu', 'sua', 'ou', 'quando', 'muito', 'nos', 'já', 'eu', 'também', 'só', 'pelo', 'pela', 'até', 'isso', 'ela', 'entre', 'depois', 'sem', 'mesmo', 'aos', 'seus', 'quem', 'nas', 'me', 'esse', 'eles', 'você', 'essa', 'num', 'nem', 'suas', 'meu', 'às', 'minha', 'numa', 'pelos', 'elas', 'qual', 'nós', 'lhe', 'deles', 'essas', 'esses', 'pelas', 'este', 'dele', 'tu', 'te', 'vocês', 'vos', 'lhes', 'meus', 'minhas', 'teu', 'tua', 'teus', 'tuas', 'nosso', 'nossa', 'nossos', 'nossas', 'dela', 'delas', 'esta', 'estes', 'estas', 'aquele', 'aquela', 'aqueles', 'aquelas', 'isto', 'aquilo', 'estou', 'está', 'estamos', 'estão', 'estive', 'esteve', 'estivemos', 'estiveram', 'estava', 'estávamos', 'estavam', 'estivera', 'estivéramos', 'esteja', 'estejamos', 'estejam', 'estivesse', 'estivéssemos', 'estivessem', 'estiver', 'estivermos', 'estiverem', 'hei', 'há', 'havemos', 'hão', 'houve', 'houvemos', 'houveram', 'houvera', 'houvéramos', 'haja', 'hajamos', 'hajam', 'houvesse', 'houvéssemos', 'houvessem', 'houver', 'houvermos', 'houverem', 'houverei', 'houverá', 'houveremos', 'houverão', 'houveria', 'houveríamos', 'houveriam', 'sou', 'somos', 'são', 'era', 'éramos', 'eram', 'fui', 'foi', 'fomos', 'foram', 'fora', 'fôramos', 'seja', 'sejamos', 'sejam', 'fosse', 'fôssemos', 'fossem', 'for', 'formos', 'forem', 'serei', 'será', 'seremos', 'serão', 'seria', 'seríamos', 'seriam', 'tenho', 'tem', 'temos', 'tém', 'tinha', 'tínhamos', 'tinham', 'tive', 'teve', 'tivemos', 'tiveram', 'tivera', 'tivéramos', 'tenha', 'tenhamos', 'tenham', 'tivesse', 'tivéssemos', 'tivessem', 'tiver', 'tivermos', 'tiverem', 'terei', 'terá', 'teremos', 'terão', 'teria', 'teríamos', 'teriam']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H46UNi2adC5u"
      },
      "source": [
        "**Tokenização**\n",
        "- iremos fazer um teste simples para remover as stopwords"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fxuo4Ft8Wb8G",
        "outputId": "40f61275-b929-4761-fe38-6942b08a672f"
      },
      "source": [
        "# importando a biblioteca\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "nltk.download('punkt') # importante também realizar o download de \"punkt\"\n",
        "# esses são os recursos necessários para realizar a tokenização"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AIgbOSbhWb2o"
      },
      "source": [
        "# criando a variável \"frase\"\n",
        "frase = 'Eu dirijo devagar porque nós queremos ver os animais.'"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "idky9O7-Wb0l",
        "outputId": "ea135f1c-9375-4a21-e244-d9e2217ea6d6"
      },
      "source": [
        "# realizando a divisão da frase criada em tokens\n",
        "tokens = word_tokenize(frase)\n",
        "\n",
        "# imprimindo a tokenização\n",
        "print(tokens)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Eu', 'dirijo', 'devagar', 'porque', 'nós', 'queremos', 'ver', 'os', 'animais', '.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VwNF-jPMWbyN",
        "outputId": "05ca7150-0e61-44b2-ee86-3d36c1fd7417"
      },
      "source": [
        "# realizando um loop simples para imprimir a frase sem os stopwords caso a frase possua\n",
        "for t in tokens:\n",
        "  if t not in stopwords:\n",
        "    print(t)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Eu\n",
            "dirijo\n",
            "devagar\n",
            "porque\n",
            "queremos\n",
            "ver\n",
            "animais\n",
            ".\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "37YclVKka__U"
      },
      "source": [
        "**PARTE 2**\n",
        "\n",
        "Outra técnica muito importante na `análise de textos`, é a **identificação da frequência e importância que uma palavra pode ter em um texto.**\n",
        "\n",
        "Existe um cálculo estatístico chamado de `TF-IDF`, essa sigla vem do inglês **Term Frequency – Inverse Document Frequency**, e quer dizer: `Frequência do termo - Frequência Inversa do Documento`. Este cálculo identifica a importância que um termo tem em um texto,\n",
        "ou seja, ele permite que você descubra quais termos são mais relevantes em um dado texto ou documento.\n",
        "\n",
        "Em linhas gerais o TF-IDF busca atribuir um valor que representa um peso para definir a importância do termo em um documento, com base na frequência em que ele ocorre (TF), compensando, porém, esse peso, caso a ocorrência desse termo seja muito grande (IDF).\n",
        "\n",
        "Em resumo, se um termo aparece algumas vezes no texto, o valor do TF-IDF aumenta, significando que aquela palavra é importante, porém se esse termo se repete bastante, esse valor é compensado, e a importância dele é diminuída.\n",
        "\n",
        "Deu pra entender o TF-IDF? Agora acompanhe um exemplo prático, para melhorar sua compreensão sobre esse assunto, utilizando o módulo `“TfidfVectorizer”` da biblioteca sklearn para calcular os valores de TF-IDF de um texto.\n",
        "\n",
        "Com o ambiente do jupyter aberto, primeiramente importe o módulo através do comando `“from sklearn.feature_extraction.text import TfidfVectorizer”`, e importe também a biblioteca pandas, fazendo `“import pandas as pd”`.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lgxTcouqWbu2"
      },
      "source": [
        "# importando as bibliotecas\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import pandas as pd"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NnlDjILDWbq_"
      },
      "source": [
        "# instanciando a variável \"texto1\" para realizar o cálculo do \"TF_IDF\"\n",
        "texto1 = 'A matemática é muito importante para compreendermos como a natureza funciona'"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G0OU0khqWbpD"
      },
      "source": [
        "# instanciando o módulo\n",
        "tf_idf = TfidfVectorizer()"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i6iQyiIfh3DV"
      },
      "source": [
        "**Usando o fit_transform**\n",
        "\n",
        "E instanciando numa variável, onde ele retornará uma matriz com os termos e os scores associados, passando o `texto1` como contéudo."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b3X4uzA2WbnH",
        "outputId": "4862df16-19c6-4bce-9761-5a500dafb17d"
      },
      "source": [
        "# aplicando o \".fit_transform\"\n",
        "vetor = tf_idf.fit_transform([texto1])\n",
        "\n",
        "# imprimindo a variável\n",
        "print(vetor) # retorna uma matriz com os termos e os scores associados"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  (0, 2)\t0.35355339059327373\n",
            "  (0, 6)\t0.35355339059327373\n",
            "  (0, 0)\t0.35355339059327373\n",
            "  (0, 1)\t0.35355339059327373\n",
            "  (0, 7)\t0.35355339059327373\n",
            "  (0, 3)\t0.35355339059327373\n",
            "  (0, 5)\t0.35355339059327373\n",
            "  (0, 4)\t0.35355339059327373\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hepTVnpGj0EF",
        "outputId": "03128a74-8ff4-4036-9f39-70e545d3d59a"
      },
      "source": [
        "# colocando a matriz em um formato de array\n",
        "vetor = vetor.todense()\n",
        "\n",
        "print(vetor)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0.35355339 0.35355339 0.35355339 0.35355339 0.35355339 0.35355339\n",
            "  0.35355339 0.35355339]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7GYeeZsSWblQ",
        "outputId": "f78f21e7-3d50-491e-87ad-000c342255a8"
      },
      "source": [
        "# obtendo os nomes das palavras mapeadas\n",
        "nomes  = tf_idf.get_feature_names()\n",
        "\n",
        "# imprimindo a variável\n",
        "print(nomes)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['como', 'compreendermos', 'funciona', 'importante', 'matemática', 'muito', 'natureza', 'para']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dNPSOavhWbhj",
        "outputId": "6384874f-7252-4cb8-de58-27887f943c15"
      },
      "source": [
        "# assim é possível associar os scores aos nomes das palavras e o resultado pode criado em dataframe\n",
        "df = pd.DataFrame(vetor, columns=nomes)\n",
        "\n",
        "print(df) # visualizando o dataframe"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "       como  compreendermos  funciona  ...     muito  natureza      para\n",
            "0  0.353553        0.353553  0.353553  ...  0.353553  0.353553  0.353553\n",
            "\n",
            "[1 rows x 8 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TK39tD6QlpCX"
      },
      "source": [
        "Analisando o resultado dos scores das palavras você pode notar que o TF-IDF deu o mesmo score para todos os termos, pois só existe uma ocorrência de cada termo.\n",
        "\n",
        "Porém se criar outro texto com o seguinte conteúdo: `“texto2 = 'A matemática é incrível, quanto mais estudo matemática, mais eu consigo aprender matemática'”`, e repetir os procedimentos feitos anteriormente para o texto1, poderá obter scores diferentes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e1gGSZ_jm5m6"
      },
      "source": [
        "**Usando um novo texto**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QPcEpidMWbd8"
      },
      "source": [
        "# instanciando o novo texto na variável \"texto2\"\n",
        "texto2 = 'A matemática é incrível, quanto mais estudo matemática, mais eu consigo aprender matemática'"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J_ztP4XgWbbw"
      },
      "source": [
        "# instanciando o módulo\n",
        "tf_idf = TfidfVectorizer()\n",
        "\n",
        "# aplicando o \".fit_transform\" na variável \"texto2\"\n",
        "vetor2 = tf_idf.fit_transform([texto2]) # retorna uma matriz com os termos e os scores associados"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Aj8smfsMWbWt"
      },
      "source": [
        "# colocando a matriz em um formato de array\n",
        "vetor2 = vetor2.todense()"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9fq-wi9omQhq"
      },
      "source": [
        "# obtendo os nomes das palavras mapeadas\n",
        "nomes = tf_idf.get_feature_names()"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rVrRdcurmQdf",
        "outputId": "f76f5e25-976d-470b-856f-cbc46f852e0b"
      },
      "source": [
        "# associando os scores aos nomes das palavras e o resultado em um dataframe\n",
        "df = pd.DataFrame(vetor2, columns=nomes)\n",
        "print(df)"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "   aprender   consigo    estudo  ...      mais  matemática    quanto\n",
            "0  0.229416  0.229416  0.229416  ...  0.458831    0.688247  0.229416\n",
            "\n",
            "[1 rows x 8 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UK94kQyFmkRc"
      },
      "source": [
        "Logo perceberá que a palavra ‘matemática’ recebe o maior score, pois ocorre 3 vezes no texto. A palavra ‘mais’ recebe o segundo maior score, pois só aparece 2 vezes, e o restante recebe o mesmo score, pois só aparecem 1 vez.\n",
        "\n",
        "Muita coisa, não é mesmo?\n",
        "\n",
        "Até aqui você pôde aprender sobre o `Processamento da Linguagem Natural` de uma forma introdutória, e conhecer alguns conceitos importantes para o processamento de texto como, **Corpus, Stopwords e Tokenização**. Também entendeu como é realizado o cálculo TF-\n",
        "IDF para verificar a importância que um termo tem dentro de um texto. \n",
        "\n",
        "Além de tudo isso, pôde acompanhar a aplicação desses conceitos utilizando Python. \n",
        "\n",
        "Então, agora é com você!\n",
        "\n",
        "Tente praticar e resolver os exercícios propostos para aprofundar seus conhecimentos.\n",
        "\n",
        "Bons estudos e até mais!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q3aydvn4mQYY"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ZEaRwbTmQV7"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DBPOg3ZPmQTW"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l9qeO7IBmQQ4"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6fXA9s7dmQOx"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sDSGge6mmQMn"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FfoqF6mfmQIh"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "169sgjORmQEa"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T7DYOOlFmQCG"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0NAkkvUemP_7"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZoBJ22PEmP9r"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6CWXaD4TmP4G"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}